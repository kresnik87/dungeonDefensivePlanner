version: "3"
services:
  dungeon:
    build: .
    networks:
      - mynet2
    environment:
        SYMFONY_ENV: dev
        BUILD_ENV: local
    volumes:
        - .:/app
    ports:
        - 5800:80

  elasticsearch:
    container_name: elasticsearch_cont
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.12.0"
    environment:
      - "discovery.type=single-node"
      - "bootstrap.memory_lock=true"
      - "ES_JAVA_OPTS=-Xms1G -Xmx1G"
      - "xpack.security.enabled=false"
      - "http.cors.enabled=true"
      - "http.cors.allow-origin=*"
    ports:
      - 9201:9200
    volumes:
      - elastic_volume:/usr/share/elasticsearch/data
  #gather all logs from different sources and pass them to elastic-search in a proper format
  logstash_cont:
    container_name: logstash_cont_dung
    build: ./docker_conf/logstash
    #ports:
    #  - 5041:5044
    depends_on:
      - elasticsearch

  #keep track of the symfony log file and send it to elastic search
  filebeats_cont:
      container_name: filebeats_cont_dung
      build: ./docker_conf/filebeats
      volumes:
        - ./var/log:/var/log/symfony
      depends_on:
        - logstash_cont

  kibana_cont:
      image: docker.elastic.co/kibana/kibana:7.12.0
      container_name: kibana_cont_dung
      ports:
        - 5602:5601
      environment:
        ELASTICSEARCH_URL: http://elasticsearch:9200
        ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'

networks:
   mynet2:
       driver: bridge
volumes:
  elastic_volume:
#   rabbitmq:
#     image: "rabbitmq:3-management"
#     environment:
#       RABBITMQ_ERLANG_COOKIE: "SWQOKODSQALRPCLNMEQG"
#       RABBITMQ_DEFAULT_USER: "rabbitmq"
#       RABBITMQ_DEFAULT_PASS: "rabbitmq"
#       RABBITMQ_DEFAULT_VHOST: "/"
#     ports:
#       - 15672:15672
#       - 5672:5672
#     volumes:
#       - rabbitmq:/var/lib/rabbitmq:cached
#     networks:
#       - mynet
#
#volumes:
#  rabbitmq:

